{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Processing and predicting target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = pd.read_csv(\"data/train.csv\")\n",
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "texts = list(tweets_data[\"text\"])\n",
    "texts_test = list(test_data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729\n"
     ]
    }
   ],
   "source": [
    "chars = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q', 'r','s','t','u','v','w','x','y','z','$']\n",
    "twins = [ char1+char2 for char1 in chars for char2 in chars]\n",
    "\n",
    "twins_dict = { twins[i]:i for i in range(len(twins)) }\n",
    "print(len(twins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_twins(texts):\n",
    "    new_texts = []\n",
    "    for text in texts:\n",
    "        text=text.lower()\n",
    "        text_tokens = text.split()\n",
    "        text_tokens = [re.sub(r'[^a-z]', '', word) for word in text_tokens] # remove special characters\n",
    "        new_texts.append(\"$\".join(text_tokens))\n",
    "    twins_tweets = []\n",
    "    for new_text in new_texts:\n",
    "        tweet_twins=[]\n",
    "        for i in range(len(new_text)-1):\n",
    "            my_twin = new_text[i:i+2]\n",
    "            tweet_twins.append(twins_dict[my_twin])\n",
    "        twins_tweets.append(tweet_twins)\n",
    "    return twins_tweets\n",
    "\n",
    "train_twins_tweets = convert_to_twins(texts)\n",
    "test_twins_tweets = convert_to_twins(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf-idf matrix for the training data and test data\n",
    "\n",
    "twin_tweet_freq = {}\n",
    "for tweet in train_twins_tweets + test_twins_tweets:\n",
    "    for twin in set(tweet):\n",
    "        if twin in twin_tweet_freq:\n",
    "            twin_tweet_freq[twin]+=1\n",
    "        else:\n",
    "            twin_tweet_freq[twin]=1\n",
    "idf = { twin:np.log(len(train_twins_tweets+test_twins_tweets)/twin_tweet_freq[twin]) for twin in twin_tweet_freq }\n",
    "\n",
    "def tf_idf(tweet):\n",
    "    tf = [ 0 for i in range(len(twins))]\n",
    "    for twin_id in set(tweet):\n",
    "        # print(twin_id)\n",
    "        tf[twin_id] = tweet.count(twin_id)/len(tweet)\n",
    "    return [ tf[twin]*idf[twin] for twin in range(len(twins)) ]\n",
    "\n",
    "train_tf_idf = [ tf_idf(tweet) for tweet in train_twins_tweets ]\n",
    "test_tf_idf = [ tf_idf(tweet) for tweet in test_twins_tweets ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest accuracy  0.652230971128609\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply Random Forest Classifier and XGBoost Classifier after splitting training data into training and validation data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train = train_tf_idf\n",
    "y_train = list(tweets_data[\"target\"])\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(train_tf_idf, tweets_data[\"target\"], test_size=0.1, random_state=42)\n",
    "\n",
    "rfc = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_val)\n",
    "print(\"Random forest accuracy \", accuracy_score(y_val, y_pred))\n",
    "\n",
    "\n",
    "xgc = XGBClassifier()\n",
    "xgc.fit(X_train, y_train)\n",
    "# y_pred = rfc.predict(X_val)\n",
    "# print(\"XGBoost accuracy \", accuracy_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "476/476 [==============================] - 0s 549us/step - loss: 0.2469 - accuracy: 0.5703\n",
      "Epoch 2/10\n",
      "476/476 [==============================] - 0s 477us/step - loss: 0.2453 - accuracy: 0.5703\n",
      "Epoch 3/10\n",
      "476/476 [==============================] - 0s 470us/step - loss: 0.2451 - accuracy: 0.5703\n",
      "Epoch 4/10\n",
      "476/476 [==============================] - 0s 471us/step - loss: 0.2451 - accuracy: 0.5703\n",
      "Epoch 5/10\n",
      "476/476 [==============================] - 0s 485us/step - loss: 0.2451 - accuracy: 0.5703\n",
      "Epoch 6/10\n",
      "476/476 [==============================] - 0s 466us/step - loss: 0.2451 - accuracy: 0.5703\n",
      "Epoch 7/10\n",
      "476/476 [==============================] - 0s 466us/step - loss: 0.2451 - accuracy: 0.5703\n",
      "Epoch 8/10\n",
      "476/476 [==============================] - 0s 462us/step - loss: 0.2451 - accuracy: 0.5703\n",
      "Epoch 9/10\n",
      "476/476 [==============================] - 0s 468us/step - loss: 0.2451 - accuracy: 0.5703\n",
      "Epoch 10/10\n",
      "476/476 [==============================] - 0s 462us/step - loss: 0.2451 - accuracy: 0.5703\n"
     ]
    }
   ],
   "source": [
    "# Implement a neural network of 2 hidden layers with ReLU activation function over the tf-idf matrix to predict the target \n",
    "#   using sequential model from keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# different axtivation layers: https://keras.io/api/layers/activations/\n",
    "model = Sequential([\n",
    "    Dense(256, input_shape=(len(twins), )),\n",
    "    Activation('softmax'),\n",
    "    Dense(64),\n",
    "    Activation('relu'),\n",
    "    Dense(1),\n",
    "    Activation('sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model.fit(np.array(X_train), np.array(y_train), epochs=10, batch_size=16)\n",
    "pickle.dump(model, open(\"neural_model.pkl\", \"wb\"))\n",
    "\n",
    "# y_pred = model.predict(np.array(X_val))\n",
    "# print(\"Neural network accuracy \", model.score(np.array(X_val), np.array(y_val)))\n",
    "\n",
    "# Create a submission file for the test data\n",
    "# y_pred = model.predict_classes(np.array(test_tf_idf))\n",
    "# y_pred = model.preict(np.array(test_tf_idf))\n",
    "# submission = pd.DataFrame()\n",
    "# submission[\"id\"] = test_data[\"id\"]\n",
    "# submission[\"target\"] = y_pred\n",
    "# submission.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 291us/step\n",
      "0.4296597924602653\n",
      "0.4241495556236592\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(np.array(test_tf_idf))\n",
    "submission = pd.DataFrame()\n",
    "submission[\"id\"] = test_data[\"id\"]\n",
    "target = np.array(y_pred) #[1 if (y_pr >= 0.5) else 0 for y_pr in y_pred]\n",
    "min_t = min(target)\n",
    "max_t = max(target)\n",
    "print(sum(y_train)/len(y_train))\n",
    "\n",
    "target = [ (val-min_t)/(max_t-min_t) for val in target]\n",
    "\n",
    "submission[\"target\"] = [1 if val>0.605 else 0 for val in target]\n",
    "print(sum(submission[\"target\"])/len(submission[\"target\"]))\n",
    "# print(submission[\"target\"])\n",
    "# max(submission[\"target\"])\n",
    "# print(min(submission[\"target\"]))\n",
    "submission.to_csv(\"data/neural_submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
